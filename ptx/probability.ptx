<section xml:id="probability">
  <title>Probability</title>
  <subsection xml:id="density-functions">
    <title>Probability Density Functions</title>
    <p>
      The final major application of integration in this chapter is the use of integration to understand continuous probability.
      When dealing with probability,
      there are two major kinds of data:
      discrete and continuous.
      In discrete data,
      there are a finite number of seperate possible measurements,
      each of which has a finite probability.
      The study of discrete probability can be entirely accomplished with finite sums (though even there,
      caluculus can give surprising insights.)
    </p>
    <p>
      Continuous probability involves measurements which can vary anywhere within a range.
      Test scores are a typical discrete measurement:
      there are only a finite number of test results.
      Heights in a population are a typical continuous measurement:
      assuming sufficient precision,
      a height can be any real number in a particular range.
      At least mathematically, there are infinitely many possible measurements.
      As opposed to discrete probability,
      we can't assign a specific likeliness to any particular measurement in a continuous situation.
      Instead, we can assign a probability to a range of measurements.
      For example:
      there is a 20% change that an adult female caribou stands more than 125 cm tall at the shoulder.
    </p>
    <definition>
      <statement>
        <p>
          A <term>probability density</term>
          is defined to be an integrable function <m>f(x) : [a,b] \rightarrow [0, \infty)</m> such that
          <me>
            \int_a^b f(x) dx = 1
          </me>.
        </p>
      </statement>
    </definition>
    <p>
      The interval <m>[a,b]</m> is the range of all possible measurements.
      The integral condition is simply saying that all measurements fall in this range
      (with probability 1).
      Then, if we have <m>x_0</m> and <m>x_1</m> in the interval <m>[a,b]</m>,
      the probability of a measurement between <m>x_0</m> and <m>x_1</m> is given by the integral of the density.
      <me>
        \int_{x_0}^{x_1} f(x) dx
      </me>
    </p>
    <p>
      We need the probability density function to be positive because negative probability doesn't make any sense.
      Note that measurements are <m>x</m> values <mdash/> inputs to the function,
      not outputs.
      <m>f(x)</m> measures, in some sense,
      the likeliness of <m>x</m> being a measurement.
      In the ensuing study of probability,
      all information about the situation is determined from the probability density function.
    </p>
    <p>
      Since it is a common convention,
      we will also refer to the probability density function as a
      <em>probability distribution</em>
      or simply a distribution.
    </p>
  </subsection>
  <subsection xml:id="normalization">
    <title>Normalization</title>
    <p>
      Often we are given a positive function <m>f(x)</m> on an interval,
      but not necessarily with the condition that its integral over the interval is one.
      We can choose parameters or multiply <m>f(x)</m> by some constant to ensure that the total integral over the interval is one;
      such a process is called normalization.
    </p>
    <example>
      <statement>
        <p>
          Let <m>\alpha > 0</m> and <m>f(x) = e^{-\alpha x}</m>.
          We consider <m>f(x)</m> a possible density function on <m>[0, \infty)</m>.
          We try to calculate its integral over this whole interval.
          <md>
            <mrow>\int_0^\infty e^{-\alpha x} dx \amp  = \lim_{a \rightarrow \infty} \int_0^a e^{-\alpha x} dx</mrow>
            <mrow>\amp  = \lim_{a \rightarrow \infty} \left. \frac{e^{-\alpha x}}{-\alpha} \right|_0^a</mrow>
            <mrow>\amp  = \lim_{a \rightarrow \infty} \frac{-e^{-\alpha a}}{\alpha} + \frac{1}{\alpha} = \frac{1}{\alpha}</mrow>
          </md>
        </p>
        <p>
          We must have a total integral of <m>1</m> for <m>f</m> to be a probability density function,
          which means that <m>\alpha = 1</m>.
          <m>f(x) = e^{-x}</m> is a probability density function on <m>[0,\infty)</m>.
          This interval means that all possible postiive values are measurements.
          The fact that <m>f</m> is a decay function means that measurements gets less and less likely as values get large.
          The probability of an event between <m>0</m> and <m>1</m> is
          <me>
            \int_0^1 e^{-x} dx = \left. e^{-x} \right|_0^1 = -e^{-1} + 1 = 1 - \frac{1}{e} \doteq 0.632
          </me>.
        </p>
        <p>
          Likewise, the probability of a measurement bewteen <m>1</m> and <m>2</m> is
          <me>
            \int_1^2 e^{-x} dx = \left. e^{-x}\right|_1^2 = -e^{-2} + \frac{1}{e} = \frac{1}{e} - \frac{1}{e^2} = \frac{e-1}{e^2} \doteq 0.233
          </me>.
        </p>
      </statement>
    </example>
    <figure xml:id="figure-probability-density">
      <caption>The Probability Density <m>e^{-x}</m></caption>
      <image width="1200%" source="images/figure27.png" />
    </figure>
    <example>
      <statement>
        <p>
          The most well-known probability density function is the bell curve,
          which is also called the gaussian distribution or normal distribution.
          In full generality it depends on two parameters <m>\mu</m> and <m>\sigma</m> and has the following form.
          <me>
            f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
          </me>
        </p>
        <p>
          First, let's consider <m>f(x) = e^{-x^2}</m> and try to normalize.
          <me>
            \int_{-\infty}^\infty e^{-x^2} dx
          </me>
        </p>
        <p>
          This is a problematic integral:
          <m>e^{-x^2}</m> has no elementary anti-derivative.
          The value of the integral is calculuced by clever techniques in multi-variable calculus;
          for us, we'll just state the value.
          <me>
            \int_{-\infty}^\infty e^{-x^2} dx = \sqrt{\pi}
          </me>
        </p>
        <p>
          This allows normalization.
          <m>f(x) = \frac{1}{\sqrt{\pi}} e^{-x^2}</m> is a probability distribution on all of <m>\RR</m>.
        </p>
      </statement>
    </example>
    <figure xml:id="figure-gaussian-distribution">
      <caption>The Gaussian Distribution <m>\frac{1}{\sqrt{\pi}}
      e^{-x^2}</m></caption>
      <image width="900%" source="images/figure26.png" />
    </figure>
    <example>
      <statement>
        <p>
          Here is another common and important example.
          <me>
            f(x) = \left\{ \begin{matrix} A \amp  x \in [a,b] \\ 0 \amp  x \notin [a,b] \end{matrix} \right.
          </me>
        </p>
        <p>
          This measure an equal probablity of all measurements in the range <m>[a,b]</m>.
          It is normalized by setting <m>A = \frac{1}{b-a}</m>.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="means">
    <title>Means</title>
    <p>
      For discrete probability,
      a mean or average of the expected measurements is relatively intuitive:
      we just add up the measurements multiplied by their probabilities.
      What a mean should be for a continuous probability isn't as immediately obvious,
      since we can't add up the infinitely-many measurements.
      However, we can still take inspiration from the discrete case.
    </p>
    <p>
      Let's consider finite probability for a moment.
      Say that there are <m>n</m> events with probabilities <m>p_i</m> and measurements <m>r_i</m>.
      Then the normalization is
      <me>
        \sum_{i=1}^n p_i = 1
      </me>.
    </p>
    <p>
      The mean is the sum of the measurements multiplied by their probabilities.
      <me>
        \sum_{i=1}^n r_i p_i
      </me>
    </p>
    <p>
      These discrete calculations give inspiration for the continuous case.
      The major difference, for continuous probability,
      is that our sums are now integrals.
      Apart from the difference, we still basically take the same steps:
      multiply by the measurement to get the mean.
    </p>
    <definition>
      <statement>
        <p>
          The average or mean of a probability density <m>f(x)</m> on <m>[a,b]</m> is defined to be the following integral.
          <me>
            \mu = \int_a^b x f(x) dx
          </me>
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          For <m>f(x) = e^{- x}</m> on <m>[0, \infty)</m>,
          this is the mean calculation
          (using integration by parts).
          <md>
            <mrow>\mu \amp  = \int_0^\infty x e^{-x} dx</mrow>
            <mrow>\amp  = \left. - x e^{-x} \right|_0^\infty + \int_0^\infty e^{-x} dx</mrow>
            <mrow>\amp  = 0 + \left. e^{-x} \right|_0^\infty = 1</mrow>
          </md>
        </p>
        <p>
          This mean makes some sense for a decay function.
          Even though very large measurements are possible,
          they become very unlikely.
          The most likely measurements are near <m>0</m>,
          so the mean works out to <m>1</m>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let's also calculate the mean for <m>\frac{1}{b-a}</m>
          (constant probability on the interval <m>[a,b]</m>).
          <md>
            <mrow>\mu \amp  = \int_a^b \frac{1}{b-a} x dx</mrow>
            <mrow>\amp  = \left. \frac{1}{b-a} \frac{x^2}{2} \right|_a^b</mrow>
            <mrow>\amp  = \frac{b^2-a^2}{2(b-a)} = \frac{b+a}{2}</mrow>
          </md>
        </p>
        <p>
          The mean is exactly halfway between the endpoints.
          Since the probability is constant, this makes perfect sense.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let's calculate the mean for the normal distribution in full detail,
          using the parameters <m>\mu</m> and <m>\sigma</m>.
          <md>
            <mrow>f(x) \amp  = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</mrow>
            <mrow>\mu \amp  = \int_{-\infty}^\infty \frac{xe^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma \sqrt{2\pi}} dx</mrow>
            <mrow>\amp  v = x-\mu</mrow>
            <mrow>\amp  = \int_{-\infty}^\infty \frac{(v+\mu)e^{-\frac{v^2}{2\sigma^2}}}{\sigma \sqrt{2\pi}} dx</mrow>
            <mrow>\amp  = \int_{-\infty}^\infty \frac{(v)e^{-\frac{v^2}{2\sigma^2}}}{\sigma \sqrt{2\pi}} dx + \int_{-\infty}^\infty \frac{(\mu)e^{-\frac{v^2}{2\sigma^2}}}{\sigma \sqrt{2\pi}} dx</mrow>
            <mrow>\amp  = 0 + \frac{\mu}{\sigma \sqrt{2\pi}} \int_{-\infty}^\infty e^{-\frac{v^2}{2\sigma^2}} dx</mrow>
            <mrow>w \amp  = \frac{v}{\sigma \sqrt{2}}</mrow>
            <mrow>\amp  = \frac{\mu}{\sigma \sqrt{2\pi}} \sigma \sqrt{2} \int_{-\infty}^\infty e^{-w^2} dw = \frac{\mu}{\sqrt{\pi}} \sqrt{\pi} = \mu</mrow>
          </md>
        </p>
        <p>
          The mean is precisely the parameter <m>\mu</m>.
          It is the <m>x</m>-value at the centre or peak of the bell curve.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="central-tendencies">
    <title>Central Tendencies</title>
    <p>
      The mean or average is only one of several possible measures of what is the most likely outcome of a measurement.
      In general, a <em>central tendency</em>
      is any mathematical calculation of a <sq>typical</sq> value;
      for most distributions,
      there are several different central tendencies which we can consider.
      It isn't always obvious which is the most appropriate.
      The three most common and most well-known central tendencies are mean (average),
      median and mode.
    </p>
    <p>
      We could compare median versus mean for income in Canada,
      and we would find that the mean is significantly higher (about $10,000) than the median.
      Which is the more appropriate?
      It is difficult to say,
      since it is a judgement call external to the mathematics.
      The mathematics doesn't give moral guidance for which type of central tendency is the best. (You could notice, however,
      that Statistics Canada reports mostly median results for income and similar financial statistics,
      since medians are less sensitive to very large outlying values).
    </p>
    <definition>
      <statement>
        <p>
          For continuous probability and probability density <m>f(x)</m> on <m>[a,b]</m>,
          the median is defined to be the unique number <m>c</m> such that
          <me>
            \int_a^c f(x) dx = \int_c^b f(x) dx = \frac{1}{2}
          </me>.
        </p>
        <p>
          Since integrals are areas under the curve,
          and the total area on <m>[a,b]</m> is <m>1</m>,
          the median is the place which exactly divdes the area under the curve into halves.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let's calculate the median for<m>f(x) = e^{-x}</m>.
          <md>
            <mrow>\int_c^\infty e^{-x} \amp  = \left. e^{-\alpha x}\right|_c^\infty</mrow>
            <mrow>\frac{1}{2} \amp  = e^{-c}</mrow>
            <mrow>-c \amp  = \ln \frac{1}{2} = - \ln 2</mrow>
            <mrow>c \amp  = \ln 2 \lt  1</mrow>
          </md>
        </p>
        <p>
          The median of this distribution is <m>\ln 2</m>,
          which is smaller than the mean of <m>1</m>.
          This is typical for distributions with a long tail on one side.
          The very high values pull up the mean, but not the median.
          This is the same reason that mean incomes are higher than median incomes:
          very high incomes pull up the mean, but not the median.
        </p>
      </statement>
    </example>
    <p>
      One of the reasons that the bell curve is very commonly used to understand probability is that it is very well behaved for central tendencies.
      Basically any central tendency you can calculate for a bell-curve will give <m>\mu</m>,
      the mean.
    </p>
  </subsection>
  <subsection xml:id="expectation-values">
    <title>Expectation Values</title>
    <p>
      If <m>f(x)</m> on <m>[a,b]</m> is a probability density,
      a common notation for the mean is <m>\left\lt x\right></m>.
      Particularly when using this notation,
      the mean is often called the expectation value of the measurement.
    </p>
    <p>
      If we have some other function which depends on the measurement,
      <m>g(x)</m>,
      we can ask:
      what is the likely outcome of this function?
    </p>
    <definition>
      <statement>
        <p>
          The likely outcome is called the expectation value of <m>g(x)</m> and is calculated by the following integral.
          <me>
            \left\lt g(x)\right> = \int_a^b g(x) f(x) dx
          </me>
        </p>
      </statement>
    </definition>
    <p>
      Modern quantum mechanics is all probability.
      Measurables such as position, velocity,
      momentum and energy are all functions on the probability space.
      The actual values referened to are not strict values, but expectation values.
      The previous expectation value definition calculates all these measurables.
      Once this interpretation is in place,
      the physics of the situation is understood by knowing the time development of the probability density
      (called the the wave function in quantum mechanics).
      Schrodinger's equation, the heart of quantum mechanics,
      is precisely the differential equation that describes the time development of the probability density.
    </p>
  </subsection>
  <subsection xml:id="standard-deviation">
    <title>Standard Deviation</title>
    <p>
      One we've chosen a central tendency, such as the mean,
      a reasonable question asks how spread-out the measurements are.
      Are we likely to get measurements very near the mean, or very far away?
      The <em>standard deviation</em>
      of a probability density function measures this:
      a low standard deviation means that most measurements are close to the mean,
      and a high standard deviation means that measurements can be very spread-out.
    </p>
    <p>
      Following the bell curve,
      let's write <m>\mu</m> for the mean of a probability density function <m>f(x)</m> on <m>[a,b]</m>.
      The distance of a measurement from the mean is given by <m>|x-\mu|</m>.
      However, statisticians have chosen instead to measure the square of this distance,
      so let's define <m>g(x) = (x-\mu)^2</m>.
      (This conveniently gets rid of the annoying absolute value.)
    </p>
    <definition>
      <statement>
        <p>
          We define the standard deviation squared of <m>f(x)</m> to be the expectation value of this <m>g(x)</m>.
          We typically use <m>\sigma</m> for the standard deviation. (The relationships of squares between sigma squared and the integral of squares should be reminiscent of the pythagorean identity.
          This is another reasons to use <m>(x-\mu)^2</m>.)
          <me>
            \sigma^2 = \left\lt  (x-\mu)^2 \right> = \int_a^b (x-\mu)^2 f(x) dx
          </me>
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          The standard deviation of <m>f(x) = e^{-x}</m> on
          <m>[0 , \infty)</m> is calculated as follows: (We will use the integrals we've already calculated for this function to simplify the calculation a bit).
          <md>
            <mrow>\sigma^2 \amp  = \int_0^\infty \left( x - 1 \right)^2 e^{-x} dx</mrow>
            <mrow>\amp  = \int_0^\infty \left( x^2 - 2x + 1 \right) e^{-x} dx = \int_0^\infty x^2 e^{-x} dx - 2 \int_0^\infty xe^{-x} + \int_0^\infty e^{-x} dx</mrow>
            <mrow>\amp  = \left. x^2 e^{-x} \right|_0^\infty + \int_0^\infty 2 x e^{-x} dx - \int_0^\infty 2 x e^{-x} dx + 1</mrow>
            <mrow>\amp  = 0 + 1</mrow>
            <mrow>\sigma \amp  = 1</mrow>
          </md>
        </p>
        <p>
          Even with the long tail of high measurements,
          the typical distance to the mean is still <m>1</m>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          The standard deviation of the constant probability
          <m>\frac{1}{b-a}</m> is a surprisingly difficult calculation. (Recall the mean is <m>\frac{a+b}{2}</m>).
          <md>
            <mrow>\sigma^2 \amp  = \int_a^b \left( x - \frac{a+b}{2} \right)^2 \frac{1}{b-a} dx</mrow>
            <mrow>\amp  = \int_a^b \frac{x^2}{b-a} - \frac{(a+b)x}{b-a} + \frac{(a+b)^2}{4(b-a)} dx</mrow>
            <mrow>\amp  = \left. \frac{x^3}{3(b-a)} \right|_a^b - \left. \frac{(a+b)x^2}{2(b-a)} \right|_a^b + \left. \frac{(a+b)^2x}{4(b-a)} \right|_a^b</mrow>
            <mrow>\amp  = \frac{b^3-a^3}{3(b-a)} - \frac{(a+b)(b^2-a^2}{2(b-a)} + \frac{(a+b)^2(b-a)}{4(b-a)}</mrow>
            <mrow>\amp  = \frac{b^2 + ab + a^2}{3} - \frac{a^2 + 2ab+ b^2}{2} + \frac{a^2 + 2ab + b^2}{4}</mrow>
            <mrow>\amp  = b^2 \left( \frac{1}{3} - \frac{1}{2} + \frac{1}{4} \right) + ab \left( \frac{1}{3} - 1 + \frac{1}{2} \right) + a^2 \left( \frac{1}{3} - \frac{1}{2} + \frac{1}{4} \right)</mrow>
            <mrow>\amp  = \frac{b^2}{12} - \frac{ab}{6} + \frac{a^2}{12} = \frac{b^2-2ab+a^2}{12} = \frac{(b-a)^2}{12}</mrow>
            <mrow>\sigma \amp  = \sqrt{ \frac{(b-a)^2}{12}} = \frac{b-a}{2\sqrt{3}}</mrow>
          </md>
        </p>
        <p>
          This is a believable result,
          since it shows some distance from the mean but is still within the interval.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Lastly, let's calculate the standard deviation of the normal distribution.
          <md>
            <mrow>\sigma^2 \amp  = \int_{-\infty}^\infty (x-\mu)^2 \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx</mrow>
            <mrow>v \amp  = x - \mu</mrow>
            <mrow>\amp  = \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^\infty v^2 e^{\frac{-v^2}{2\sigma^2}} dv</mrow>
            <mrow>w \amp  = \frac{v}{\sigma \sqrt{2}}</mrow>
            <mrow>\amp  = \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^\infty \sigma^2 2 w^2 e^{-w^2} dw = \frac{2\sigma^2}{\sqrt{\pi}} \int_{-\infty}^\infty w^2 e^{-w^2} dw</mrow>
            <mrow>\amp  = \frac{2\sigma^2}{\sqrt{\pi}} \left[ \left. \frac{w(e^{-w^2})}{2} \right|_{-\infty}^\infty + \int_{-\infty}^\infty e^{-w^2}{2} dw \right]</mrow>
            <mrow>\amp  = \frac{\sigma^2}{\sqrt{\pi}} \int_{-\infty}^\infty e^{-w^2}dw = \frac{\sigma^2}{\sqrt{\pi}} \sqrt{\pi} = \sigma^2</mrow>
            <mrow>\sigma \amp  = \sigma</mrow>
          </md>
        </p>
        <p>
          We were being a bit lazy with notation here,
          since we already used sigma in the definition of the normal distribution.
          We see here that the notation was justified:
          the parameters <m>\mu</m> and <m>\sigma</m> in the general form of the normal distribution were precisely the mean and the standard deviation.
        </p>
      </statement>
    </example>
  </subsection>
</section>